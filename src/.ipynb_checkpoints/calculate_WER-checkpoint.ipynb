{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import inflect\n",
    "from text2digits import text2digits\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from nltk import ngrams\n",
    "import math\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "base_folder = os.getcwd()+'/' #'~/fair-speech/release/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import snippet transcripts and demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VOC transcript snippets\n",
    "\n",
    "voc_snippets = pd.read_csv(base_folder + 'inputs/VOC/voc_snippets.tsv', sep='\\t', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct VOC metadata for mis-spellings of names\n",
    "voc_metadata = pd.read_csv(base_folder + 'inputs/VOC/CSVOC_demo_info.csv', index_col=None)\n",
    "\n",
    "voc_metadata['update_speaker_code'] = voc_metadata['speaker_code']\n",
    "\n",
    "voc_metadata['update_speaker_code'] = np.where(voc_metadata['speaker_code'] == 'HUM_McLoughlin_Rachel', 'HUM_McLaughlin_Rachel', voc_metadata['update_speaker_code'])\n",
    "voc_metadata['update_speaker_code'] = np.where(voc_metadata['speaker_code'] == 'SAC_Vrilakas_Ron', 'SAC_Vrikalis_Ron', voc_metadata['update_speaker_code'])\n",
    "voc_metadata['update_speaker_code'] = np.where(voc_metadata['speaker_code'] == 'SAC_Arghittu_Allen', 'SAC_Argittu_Allen', voc_metadata['update_speaker_code'])\n",
    "voc_metadata['update_speaker_code'] = np.where(voc_metadata['speaker_code'] == 'SAC_Werner_Savannah', 'SAC_WernerRitchie_Savannah', voc_metadata['update_speaker_code'])\n",
    "voc_metadata['update_speaker_code'] = np.where(voc_metadata['speaker_code'] == 'HUM_Yoho_Meadow ', 'HUM_Yoho_Meadow', voc_metadata['update_speaker_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge VOC data with demographic attributes\n",
    "\n",
    "voc_df = voc_snippets.merge(voc_metadata, left_on=['basefile'], right_on=['update_speaker_code'], how = 'left')\n",
    "voc_df = voc_df[['basefile', 'start_time', 'end_time', 'content', 'age_interview', 'gender',\n",
    "       'race_ethnicity', 'recording_quality', 'duration', 'segment_filename']]\n",
    "voc_df.columns = ['basefile', 'start_time', 'end_time', 'content', 'age', 'gender',\n",
    "       'race_ethnicity', 'recording_quality', 'duration', 'segment_filename']\n",
    "voc_df['source'] = voc_df['segment_filename'].str[:3]\n",
    "\n",
    "# Generate separate dataframes for each interview location\n",
    "humboldt_df = voc_df[voc_df['source']=='HUM']\n",
    "sacramento_df = voc_df[voc_df['source']=='SAC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORAAL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CORAAL transcript snippets\n",
    "\n",
    "coraal_snippets = pd.read_csv(base_folder + 'inputs/CORAAL/coraal_snippets.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CORAAL audio quality usability \n",
    "\n",
    "coraal_audio_quality = pd.read_csv(base_folder + 'inputs/CORAAL/AAVE_audio_quality.csv', index_col=None)\n",
    "coraal_df = coraal_snippets.merge(coraal_audio_quality, left_on = ['basefile'], right_on = ['file_name'], how = 'left')\n",
    "coraal_df['source'] = coraal_df['segment_filename'].str[:3]\n",
    "coraal_df['quality'] = np.where(coraal_df['source'] == 'ROC', 'usable', coraal_df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CORAAL demographics from metadata\n",
    "\n",
    "filenames = [base_folder + 'inputs/CORAAL/' + 'DCB_metadata_2018.10.06.txt',\n",
    "            base_folder + 'inputs/CORAAL/' + 'PRV_metadata_2018.10.06.txt',\n",
    "            base_folder + 'inputs/CORAAL/' + 'ROC_metadata_2018.10.06.txt']\n",
    "\n",
    "coraal_metadata = pd.concat([pd.read_csv(filename, sep='\\t') for filename in filenames], sort=False)\n",
    "coraal_sub_meta = coraal_metadata[coraal_metadata['CORAAL.Spkr']==coraal_metadata['CORAAL.File'].str[:-2]]\n",
    "\n",
    "coraal_df = coraal_df.merge(coraal_sub_meta, left_on=['basefile'], right_on=['CORAAL.File'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include CORAAL racial attribute\n",
    "coraal_df = coraal_df[['basefile', 'start_time', 'end_time', 'content', 'Age', 'Gender',\n",
    "       'quality', 'duration', 'segment_filename', 'source']]\n",
    "coraal_df.columns = ['basefile', 'start_time', 'end_time', 'content', 'age', 'gender',\n",
    "       'recording_quality', 'duration', 'segment_filename', 'source']\n",
    "coraal_df['race_ethnicity'] = 'Black'\n",
    "\n",
    "# Generate separate dataframes for each interview location\n",
    "dcb_df = coraal_df[coraal_df['source']=='DCB']\n",
    "prv_df = coraal_df[coraal_df['source']=='PRV']\n",
    "roc_df = coraal_df[coraal_df['source']=='ROC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import ASR transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IBM, Amazon, Google, and Apple transcriptions\n",
    "\n",
    "def import_asr_transcripts(asr):\n",
    "    paths = ['inputs/CORAAL/dcb_'+asr+'_transcribe_5.csv',\n",
    "             'inputs/CORAAL/prv_'+asr+'_transcribe_5.csv',\n",
    "             'inputs/CORAAL/roc_'+asr+'_transcribe_5.csv',\n",
    "             'inputs/VOC/humboldt_'+asr+'_transcribe_5.csv',\n",
    "             'inputs/VOC/sacramento_'+asr+'_transcribe_5.csv']\n",
    "    dcb_csv = pd.DataFrame()\n",
    "    prv_csv = pd.DataFrame()\n",
    "    roc_csv = pd.DataFrame()\n",
    "    humboldt_csv = pd.DataFrame()\n",
    "    sacramento_csv = pd.DataFrame()\n",
    "\n",
    "    csv = [dcb_csv, prv_csv, roc_csv, humboldt_csv, sacramento_csv]\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        pathlink = base_folder + paths[i]\n",
    "        trans = csv[i]\n",
    "        trans = pd.read_csv(pathlink,\n",
    "               names = ['index', 'segment_filename', asr+'_transcription'])\n",
    "        print(len(trans))\n",
    "        trans['source'] = trans['segment_filename'].str[:3]\n",
    "        trans = trans[['source','segment_filename', asr+'_transcription']]\n",
    "        csv[i] = trans\n",
    "    transcripts = pd.concat(csv, axis=0)\n",
    "    \n",
    "    return transcripts\n",
    "\n",
    "print(\"---Microsoft----\")\n",
    "msft_transcripts = import_asr_transcripts('msft')\n",
    "print(\"---Google----\")\n",
    "google_transcripts = import_asr_transcripts('google')\n",
    "print(\"---IBM----\")\n",
    "ibm_transcripts = import_asr_transcripts('ibm')\n",
    "print(\"---Amazon----\")\n",
    "amazon_transcripts = import_asr_transcripts('amazon')\n",
    "print(\"---Apple----\")\n",
    "apple_transcripts = import_asr_transcripts('ios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge human transcripts with ASR transcripts\n",
    "\n",
    "human_transcripts = pd.concat([coraal_df, voc_df], axis=0, sort=False)\n",
    "\n",
    "asr_transcripts = msft_transcripts.merge(ibm_transcripts, left_on=['source','segment_filename'], \n",
    "                                         right_on=['source','segment_filename'], how='left')\n",
    "asr_transcripts = asr_transcripts.merge(amazon_transcripts, left_on=['source','segment_filename'], right_on=['source','segment_filename'], how='left')\n",
    "asr_transcripts = asr_transcripts.merge(google_transcripts, left_on=['source','segment_filename'], right_on=['source','segment_filename'], how='left')\n",
    "asr_transcripts = asr_transcripts.merge(apple_transcripts, left_on=['source','segment_filename'], right_on=['source','segment_filename'], how='left')\n",
    "\n",
    "all_transcripts = human_transcripts.merge(asr_transcripts, left_on=['source','segment_filename'], \n",
    "                                         right_on=['source','segment_filename'], how='left')\n",
    "\n",
    "# Relabel columns\n",
    "all_transcripts.columns = ['basefile', 'start_time', 'end_time', 'content', 'age', 'gender',\n",
    "       'recording_quality', 'duration', 'segment_filename', 'source',\n",
    "       'race_ethnicity', 'msft_transcription', 'ibm_transcription',\n",
    "       'amazon_transcription', 'google_transcription', 'apple_transcription']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restrict snippets to relevant subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict snippets to only white and black ethnicity\n",
    "white_black_snippets = all_transcripts[(all_transcripts['race_ethnicity']=='White') | (\n",
    "    all_transcripts['race_ethnicity']=='Black')]\n",
    "\n",
    "# Restrict snippets to usable recording quality\n",
    "usable_snippets = white_black_snippets[(white_black_snippets['recording_quality']=='usable')]\n",
    "\n",
    "# Restrict snippets to speakers over age 18\n",
    "usable_snippets = usable_snippets[usable_snippets['age']>=18]\n",
    "print(len(usable_snippets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential variant: no proper noun locations\n",
    "\n",
    "location_words = ['princeville', 'tarboro', 'edgecombe', 'landover',\n",
    "                  'humboldt', 'humbolt', 'wiyot', 'hydesville', 'loleta', 'redding', 'hoopa', 'weitchpec', \n",
    "                  'garberville', 'mendocino', 'mckinleyville', 'arcata', 'arcada', 'tolowa', 'orangevale', \n",
    "                  'yurok', 'eureka', 'klamath', 'obispo']\n",
    "pattern = '|'.join([r'(?i)'+loc for loc in location_words])\n",
    "no_location_snippets = usable_snippets[usable_snippets['content'].str.contains(pattern) == False]\n",
    "# using 'no_location_snippets' rather than 'usable_snippets' yields similar results, despite the fact that \n",
    "# the majority of difficult-to-spell locations are uttered by white speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean human transcriptions for CORAAL and VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any text within markers, e.g. 'We(BR) went' -> 'We went'\n",
    "\n",
    "def remove_markers(line, markers):\n",
    "    ## markers = list of pairs, e.g. ['()', '[]'] denoting breath or noise in transcripts\n",
    "    for s, e in markers:\n",
    "         line = re.sub(\" ?\\\\\" + s + \"[^\" + e + \"]+\\\\\" + e, \"\", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean CORAAL human transcript\n",
    "\n",
    "def clean_coraal(baseline_snippets):\n",
    "    # Restrict to CORAAL rows\n",
    "    baseline_coraal = baseline_snippets[baseline_snippets['race_ethnicity']=='Black']\n",
    "    \n",
    "    # Replace original unmatched CORAAL transcript square brackets with squiggly bracket\n",
    "    baseline_coraal.loc[:,'clean_content'] = baseline_coraal.loc[:,'content'].copy()\n",
    "    baseline_coraal.loc[:,'clean_content'] = baseline_coraal['clean_content'].str.replace('\\[','\\{')\n",
    "    baseline_coraal.loc[:,'clean_content'] = baseline_coraal['clean_content'].str.replace('\\]','\\}')\n",
    "    \n",
    "    def clean_within_coraal(text):\n",
    "\n",
    "        # Relabel CORAAL words. For consideration: aks -> ask?\n",
    "        split_words = text.split()\n",
    "        split_words = [x if x != 'busses' else 'buses' for x in split_words]\n",
    "        split_words = [x if x != 'aks' else 'ask' for x in split_words]\n",
    "        split_words = [x if x != 'aksing' else 'asking' for x in split_words]\n",
    "        split_words = [x if x != 'aksed' else 'asked' for x in split_words]\n",
    "        text = ' '.join(split_words)\n",
    "        \n",
    "        # remove CORAAL unintelligible flags\n",
    "        text = re.sub(\"\\/(?i)unintelligible\\/\",'',''.join(text))\n",
    "        text = re.sub(\"\\/(?i)inaudible\\/\",'',''.join(text))\n",
    "        text = re.sub('\\/RD(.*?)\\/', '',''.join(text))\n",
    "        text = re.sub('\\/(\\?)\\1*\\/', '',''.join(text))\n",
    "        \n",
    "        # remove nonlinguistic markers\n",
    "        text = remove_markers(text, ['<>', '()', '{}'])\n",
    "\n",
    "        return text\n",
    "\n",
    "    baseline_coraal['clean_content'] = baseline_coraal.apply(lambda x: clean_within_coraal(x['clean_content']), axis=1)\n",
    "    \n",
    "    return baseline_coraal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VOC human transcript\n",
    "\n",
    "def clean_voc(baseline_snippets):\n",
    "    # Restrict to CORAAL rows\n",
    "    baseline_voc = baseline_snippets[baseline_snippets['race_ethnicity']=='White']\n",
    "    \n",
    "    pre_list = ['thier', 'humbolt', 'arcada', 'ninteen', 'marajuana', 'theatre', 'portugeuse', 'majorca']\n",
    "    post_list = ['their', 'Humboldt', 'Arcata', 'nineteen', 'marijuana', 'theater', 'portuguese', 'mallorca']\n",
    "    def clean_within_voc(text):\n",
    "\n",
    "        # Relabel misspellings\n",
    "        split_words = text.split()\n",
    "        for i in range(len(pre_list)):\n",
    "            split_words = [x if x.lower() != pre_list[i] else post_list[i] for x in split_words]\n",
    "        text = ' '.join(split_words)   \n",
    "        #new_words = [x if x != 'thier' else 'their' for x in split_words]\n",
    "\n",
    "        # remove nonlinguistic markers\n",
    "        text = remove_markers(text, ['<>', '{}'])\n",
    "\n",
    "        return text\n",
    "\n",
    "    baseline_voc['clean_content'] = baseline_voc.apply(lambda x: clean_within_voc(x['content']), axis=1)\n",
    "    \n",
    "    return baseline_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply cleaning rules to both ASR and human-generated transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize state abbreviations\n",
    "\n",
    "states = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}\n",
    "\n",
    "def fix_state_abbrevs(text):\n",
    "    ix = 0\n",
    "    state_result = []\n",
    "    wordlist = text.split()\n",
    "    while ix < len(wordlist):\n",
    "        word = wordlist[ix].lower().capitalize()\n",
    "        if word in states.keys(): # is this correct check?\n",
    "            new_word = states[word]\n",
    "        elif (ix < len(wordlist)-1) and ((word + ' ' + wordlist[ix+1].lower().capitalize()) in states.keys()):\n",
    "            new_word = states[(word + ' ' + wordlist[ix+1].lower().capitalize())]\n",
    "            ix += 1\n",
    "        else:\n",
    "            new_word = word\n",
    "        state_result.append(new_word)\n",
    "        ix += 1\n",
    "    text = ' '.join(state_result)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize number parsing and dollars\n",
    "\n",
    "p = inflect.engine()\n",
    "t2d = text2digits.Text2Digits()\n",
    "\n",
    "def fix_numbers(text):\n",
    "    split_words_num = text.split()\n",
    "    new_list = []\n",
    "    for i in range(len(split_words_num)):\n",
    "        x = split_words_num[i]\n",
    "        \n",
    "        # deal with years\n",
    "        if x.isdigit():\n",
    "            if (1100 <= int(x) < 2000) or (2010 <= int(x) < 2100) or (int(x) == 5050):\n",
    "                # deal with years as colloquially spoken\n",
    "                new_word = p.number_to_words(x[:2]) + \" \" + p.number_to_words(x[2:])\n",
    "            elif \"and\" in p.number_to_words(x):\n",
    "                # remove 'and' from e.g. 'four hundred and ninety five'\n",
    "                output = p.number_to_words(x)\n",
    "                resultwords  = [word for word in output.split() if word not in ['and']]    \n",
    "                new_word = ' '.join(resultwords)\n",
    "            else:\n",
    "                new_word = p.number_to_words(x)\n",
    "            \n",
    "        # deal with cases like 1st, 2nd, etc.\n",
    "        elif re.match(r\"(\\d+)(\\w+)\", x, re.I):\n",
    "            single_digits = ['1st', '2nd', '3rd', '5th', '8th', '9th']\n",
    "            double_digits = ['12th']\n",
    "            single_num = ['1', '2', '3', '5', '8', '9']\n",
    "            double_num = ['12']\n",
    "            single_digit_labels = ['first', 'second', 'third', 'fifth', 'eighth', 'ninth']\n",
    "            double_digit_labels = ['twelfth']\n",
    "            all_digits = single_digits + double_digits\n",
    "            all_labels = single_digit_labels + double_digit_labels\n",
    "            if x in all_digits:\n",
    "                new_word = all_labels[all_digits.index(x)]\n",
    "            else:\n",
    "                items = re.match(r\"(\\d+)(\\w+)\", x, re.I).groups()\n",
    "                if (items[1] not in ['s', 'th', 'st', 'nd', 'rd']):\n",
    "                    new_word = fix_numbers(items[0]) + \" \" + items[1]\n",
    "                elif (items[0][-2:] in double_num):\n",
    "                    new_word = fix_numbers(str(100*int(items[0][:-2]))) + \" \" + fix_numbers(items[0][-2:]+items[1])\n",
    "                elif ((items[0][-1:] in single_num) and items[0][-2:-1] != '1'):\n",
    "                    try:\n",
    "                        new_word = fix_numbers(str(10*int(items[0][:-1]))) + \" \" + fix_numbers(items[0][-1:]+items[1])\n",
    "                    except:\n",
    "                        new_word = fix_numbers(items[0]) + items[1]\n",
    "                # deal with case e.g. 80s\n",
    "                elif (items[1] in ['s', 'th']) and (p.number_to_words(items[0])[-1] == 'y'):\n",
    "                    new_word = fix_numbers(items[0])[:-1] + \"ie\" + items[1]\n",
    "                else:\n",
    "                    new_word = fix_numbers(items[0]) + items[1]\n",
    "                    \n",
    "        # deal with dollars\n",
    "        elif re.match(r\"\\$[^\\]]+\", x, re.I):\n",
    "            # deal with $ to 'dollars'\n",
    "            money = fix_numbers(x[1:])\n",
    "            if x[1:] in [\"1\", \"a\"]:\n",
    "                new_word = money + \" dollar\"\n",
    "            else:\n",
    "                new_word = money + \" dollars\"\n",
    "                \n",
    "        elif re.match(r\"\\£[^\\]]+\", x, re.I):\n",
    "            # deal with £ to 'pounds'\n",
    "            money = fix_numbers(x[1:])\n",
    "            if x[1:] in [\"1\", \"a\"]:\n",
    "                new_word = money + \" pound\"\n",
    "            else:\n",
    "                new_word = money + \" pounds\"\n",
    "                \n",
    "        else:\n",
    "            new_word = x       \n",
    "        \n",
    "        new_list.append(new_word)\n",
    "        \n",
    "    text = ' '.join(new_list)\n",
    "    text =re.sub(r'[^\\s\\w$]|_', ' ',text)\n",
    "    \n",
    "    # Deal with written out years (two thousand and ten -> twenty ten)\n",
    "    for double_dig in range(10, 100):\n",
    "        double_dig_str = p.number_to_words(double_dig)\n",
    "        text = re.sub('two thousand and ' + double_dig_str, 'twenty ' + double_dig_str, text.lower())\n",
    "        text = re.sub('two thousand ' + double_dig_str, 'twenty ' + double_dig_str, text.lower())\n",
    "\n",
    "    # Change e.g. 101 to 'one oh one' -- good for area codes\n",
    "    single_dig_list = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    for j in single_dig_list:\n",
    "        text = re.sub('thousand and ' + j, 'thousand ' + j, text.lower())\n",
    "        for k in single_dig_list:\n",
    "            #print(j + ' hundred ' + k)\n",
    "            text = re.sub(j + ' hundred ' + k + ' ', j + ' oh ' + k + ' ', text.lower())\n",
    "            text = re.sub(j + ' hundred ' + k + '$', j + ' oh ' + k, text.lower())\n",
    "    \n",
    "    text = re.sub(\"\\s+\",\" \",''.join(text)) # standardize whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text cleaning on all ASR transcriptions, as well as human transcriptions of VOC and CORAAL\n",
    "\n",
    "def clean_all_transcripts(baseline_snippets):\n",
    "\n",
    "    new_baseline = baseline_snippets.copy()\n",
    "    new_baseline['google_transcription'] = new_baseline['google_transcription'].replace(np.nan, '', regex=True)  \n",
    "    new_baseline['ibm_transcription'] = new_baseline['ibm_transcription'].replace(np.nan, '', regex=True)  \n",
    "    new_baseline['amazon_transcription'] = new_baseline['amazon_transcription'].replace(np.nan, '', regex=True)  \n",
    "    new_baseline['msft_transcription'] = new_baseline['msft_transcription'].replace(np.nan, '', regex=True)  \n",
    "    new_baseline['apple_transcription'] = new_baseline['apple_transcription'].replace(np.nan, '', regex=True)  \n",
    "    \n",
    "    swear_words = ['nigga', 'niggas', 'shit', 'bitch', 'damn', 'fuck', 'fuckin', 'fucking', 'motherfuckin', 'motherfucking']\n",
    "    filler_words = ['um', 'uh', 'mm', 'hm', 'ooh', 'woo', 'mhm', 'huh', 'ha']\n",
    "    \n",
    "    pre_cardinal = ['N', 'E', 'S', 'W', 'NE', 'NW', 'SE', 'SW']\n",
    "    post_cardinal = ['North', 'East', 'South', 'West', 'Northeast', 'Northwest', 'Southeast', 'Southwest']\n",
    "    \n",
    "    pre_list = ['cuz', 'ok', 'o', 'till', 'yup', 'imma', 'mister', 'doctor',\n",
    "                'gonna', 'tryna',\n",
    "               'carryout', 'sawmill', 'highschool', 'worldclass',\n",
    "               'saint', 'street', 'state',\n",
    "                'avenue', 'road', 'boulevard',\n",
    "               'theatre', 'neighbour', 'neighbours', 'neighbourhood', 'programme']\n",
    "    post_list = ['cause', 'okay', 'oh', 'til', 'yep', 'ima', 'mr', 'dr',\n",
    "                 'going to', 'trying to',\n",
    "                'carry out', 'saw mill', 'high school', 'world class',\n",
    "                 'st', 'st', 'st',\n",
    "                 'ave', 'rd', 'blvd',\n",
    "                 'theater', 'neighbor', 'neighbors', 'neighborhood', 'program']\n",
    "\n",
    "    def clean_within_all(text):\n",
    "        \n",
    "        # remove hesitation from IBM transcript\n",
    "        text = re.sub('%HESITATION',' ',''.join(text))\n",
    "        \n",
    "        # fix spacing in certain spellings\n",
    "        text = re.sub('T V','TV',''.join(text))\n",
    "        text = re.sub('D C','DC',''.join(text))\n",
    "        \n",
    "        # remove remaining floating non-linguistic words\n",
    "        single_paren = ['<','>', '(',')', '{','}','[',']']\n",
    "        for paren in single_paren:\n",
    "            linguistic_words  = [word for word in text.split() if paren not in word]    \n",
    "            text = ' '.join(linguistic_words)\n",
    "              \n",
    "        # general string cleaning\n",
    "        text = re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", text , 0, re.IGNORECASE) # replace inter-word hyphen with space\n",
    "        text = re.sub(\"'\",'',''.join(text)) # remove apostrophe\n",
    "        text =re.sub(r'[^\\s\\w$]|_', ' ',text) # replace special characters with space, except $\n",
    "        text = re.sub(\"\\s+\",\" \",''.join(text)) # standardize whitespace\n",
    "        \n",
    "        # update numeric numbers to strings and remove $\n",
    "        text = re.sub(\"ft ²\", \"square feet\", ''.join(text))\n",
    "        text = fix_numbers(text)\n",
    "        text = re.sub(\"\\$\",'dollars',''.join(text))\n",
    "        text = re.sub(\"\\£\",'pounds',''.join(text))\n",
    "        \n",
    "        # standardize spellings\n",
    "        split_words = text.split()\n",
    "        for i in range(len(pre_list)):\n",
    "            split_words = [x if x.lower() != pre_list[i] else post_list[i] for x in split_words]\n",
    "        text = ' '.join(split_words)        \n",
    "        \n",
    "        # deal with cardinal directions\n",
    "        split_words_dir = text.split()\n",
    "        for i in range(len(pre_cardinal)):\n",
    "            split_words_dir = [x if x != pre_cardinal[i] else post_cardinal[i] for x in split_words_dir]\n",
    "        text = ' '.join(split_words_dir)\n",
    "        \n",
    "        # deal with state abbreviations\n",
    "        text = fix_state_abbrevs(text)\n",
    "        text = text.lower()\n",
    "   \n",
    "        # update spacing in certain spellings\n",
    "        spacing_list_pre = ['north east', 'north west', 'south east', 'south west', 'all right']\n",
    "        spacing_list_post = ['northeast', 'northwest', 'southeast', 'southwest', 'alright']\n",
    "        for i in range(len(spacing_list_pre)):\n",
    "            text = re.sub(spacing_list_pre[i], spacing_list_post[i],''.join(text))\n",
    "\n",
    "        # remove filler words and swear words\n",
    "        remove_words = swear_words + filler_words\n",
    "        resultwords  = [word for word in text.split() if word not in remove_words]    \n",
    "        result = ' '.join(resultwords)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    new_baseline['clean_content'] = new_baseline.apply(lambda x: clean_within_all(x['clean_content']), axis=1)\n",
    "    new_baseline['clean_google'] = new_baseline.apply(lambda x: clean_within_all(x['google_transcription']), axis=1)\n",
    "    new_baseline['clean_ibm'] = new_baseline.apply(lambda x: clean_within_all(x['ibm_transcription']), axis=1)\n",
    "    new_baseline['clean_amazon'] = new_baseline.apply(lambda x: clean_within_all(x['amazon_transcription']), axis=1)\n",
    "    new_baseline['clean_msft'] = new_baseline.apply(lambda x: clean_within_all(x['msft_transcription']), axis=1)\n",
    "    new_baseline['clean_apple'] = new_baseline.apply(lambda x: clean_within_all(x['apple_transcription']), axis=1)\n",
    "\n",
    "    return new_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning rules to CORAAL and VOC\n",
    "\n",
    "def clean_everything(df):\n",
    "    coraal_usable = clean_coraal(df)\n",
    "    voc_usable = clean_voc(df)\n",
    "    all_usable = pd.concat([coraal_usable, voc_usable], axis=0)\n",
    "    clean_all = clean_all_transcripts(all_usable)\n",
    "    return clean_all\n",
    "\n",
    "clean_usable_snippets = clean_everything(usable_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-cleaning restriction to snippets that have more than 5 words\n",
    "\n",
    "clean_usable_snippets['wordcount'] = clean_usable_snippets['clean_content'].str.split().str.len()\n",
    "clean_usable_snippets = clean_usable_snippets[clean_usable_snippets['wordcount']>=5]\n",
    "print(len(clean_usable_snippets))\n",
    "\n",
    "# Final restriction to rows without word list uttered as part of cleaned speech\n",
    "\n",
    "word_list_snippets = ['SAC_Sindle_Rhea_3652225_3692307.wav', 'SAC_Wyley_Hannah_3361328_3405912.wav']\n",
    "clean_usable_snippets = clean_usable_snippets[~clean_usable_snippets['segment_filename'].isin(word_list_snippets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unintelligible snippets in each of CORAAL and VOC datasets\n",
    "\n",
    "black_counts = clean_usable_snippets[clean_usable_snippets['race_ethnicity']=='Black']\n",
    "white_counts = clean_usable_snippets[clean_usable_snippets['race_ethnicity']=='White']\n",
    "    \n",
    "black_unintel_regex = '\\/unintelligible\\/|\\/inaudible\\/|\\/RD(.*?)\\/|\\/(\\?)\\1*\\/'\n",
    "black_unintel = black_counts[black_counts['content'].str.contains(black_unintel_regex)]\n",
    "\n",
    "white_unintel_regex = '\\(\\(\\)\\)|\\(\\((\\s)\\1*\\)\\)'\n",
    "white_unintel = white_counts[white_counts['content'].str.contains(white_unintel_regex)]\n",
    "\n",
    "print(\"Black unintelligible snippets: \", len(black_unintel), \" out of \", len(black_counts), \"which is \", \n",
    "      100*len(black_unintel)/len(black_counts), \"%\")\n",
    "print(\"White unintelligible snippets: \", len(white_unintel), \" out of \", len(white_counts), \"which is \", \n",
    "      100*len(white_unintel)/len(white_counts), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WER Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER\n",
    "\n",
    "def wer_calc(transcripts, human_clean_col, asr_clean_col):\n",
    "    new_transcripts = transcripts.copy()\n",
    "    temp1 = transcripts[human_clean_col].tolist()\n",
    "    for col in asr_clean_col:\n",
    "        new_transcripts[col] = new_transcripts[col].replace(np.nan, '', regex=True)\n",
    "        temp2 = new_transcripts[col].tolist()\n",
    "        wer_list = []\n",
    "        for i in range(len(temp1)):\n",
    "            wer_list.append(wer(temp1[i], temp2[i]))\n",
    "        new_transcripts[col+\"_wer\"] = wer_list\n",
    "\n",
    "    return new_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ASR list for WER calculations\n",
    "\n",
    "clean_asr_trans_list = ['clean_google',\n",
    "                        'clean_ibm',\n",
    "                        'clean_amazon',\n",
    "                        'clean_msft',\n",
    "                        'clean_apple']\n",
    "\n",
    "# Run WER calculations on all usable snippets, with cleaning\n",
    "clean_transcripts_wer = wer_calc(clean_usable_snippets, 'clean_content', clean_asr_trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export WER calculations\n",
    "\n",
    "clean_transcripts_wer.to_csv(base_folder + 'output/transcribed_wer_usable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split WER dataset into AAVE and white speakers\n",
    "\n",
    "def split_white_black(all_trans):\n",
    "    black_stack = all_trans[all_trans['race_ethnicity']=='Black']\n",
    "    white_stack = all_trans[all_trans['race_ethnicity']=='White']\n",
    "    return black_stack, white_stack\n",
    "\n",
    "clean_black_stack, clean_white_stack = split_white_black(clean_transcripts_wer)\n",
    "print(len(clean_black_stack))\n",
    "print(len(clean_white_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include separate (less heavy) cleaning for LM perplexity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean CORAAL human transcript -- ignore cases & punctuation\n",
    "\n",
    "def apostrophe_clean_coraal(baseline_snippets):\n",
    "    baseline_coraal = baseline_snippets\n",
    "    # Replace original unmatched CORAAL transcript square brackets with squiggly bracket\n",
    "    baseline_coraal.loc[:,'apostrophe_clean_content'] = baseline_coraal.loc[:,'content'].copy()\n",
    "    baseline_coraal.loc[:,'apostrophe_clean_content'] = baseline_coraal['apostrophe_clean_content'].str.replace('\\[','\\{')\n",
    "    baseline_coraal.loc[:,'apostrophe_clean_content'] = baseline_coraal['apostrophe_clean_content'].str.replace('\\]','\\}')\n",
    "    \n",
    "    def apostrophe_clean_within_coraal(text):\n",
    "\n",
    "        # Relabel CORAAL words. For consideration: aks -> ask?\n",
    "        split_words = text.split()\n",
    "        split_words = [x if x != 'busses' else 'buses' for x in split_words]\n",
    "        split_words = [x if x != 'aks' else 'ask' for x in split_words]\n",
    "        split_words = [x if x != 'aksing' else 'asking' for x in split_words]\n",
    "        split_words = [x if x != 'aksed' else 'asked' for x in split_words]\n",
    "        text = ' '.join(split_words)\n",
    "        \n",
    "        # remove CORAAL unintelligible flags\n",
    "        text = re.sub(\"\\/(?i)unintelligible\\/\",'',''.join(text))\n",
    "        text = re.sub(\"\\/(?i)inaudible\\/\",'',''.join(text))\n",
    "        text = re.sub('\\/RD(.*?)\\/', '',''.join(text))\n",
    "        text = re.sub('\\/(\\?)\\1*\\/', '',''.join(text))\n",
    "        \n",
    "        # remove nonlinguistic markers\n",
    "        text = remove_markers(text, ['<>', '()', '{}'])\n",
    "\n",
    "        return text\n",
    "\n",
    "    baseline_coraal['apostrophe_clean_content'] = baseline_coraal.apply(lambda x: apostrophe_clean_within_coraal(x['apostrophe_clean_content']), axis=1)\n",
    "    \n",
    "    return baseline_coraal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VOC human transcript\n",
    "\n",
    "def apostrophe_clean_voc(baseline_snippets):\n",
    "    # Restrict to CORAAL rows\n",
    "    baseline_voc = baseline_snippets\n",
    "    \n",
    "    pre_list = ['thier', 'humbolt', 'arcada', 'ninteen', 'marajuana', 'theatre', 'portugeuse', 'majorca']\n",
    "    post_list = ['their', 'Humboldt', 'Arcata', 'nineteen', 'marijuana', 'theater', 'portuguese', 'mallorca']\n",
    "    def apostrophe_clean_within_voc(text):\n",
    "\n",
    "        # Relabel misspellings\n",
    "        split_words = text.split()\n",
    "        for i in range(len(pre_list)):\n",
    "            split_words = [x if x.lower() != pre_list[i] else post_list[i] for x in split_words]\n",
    "        text = ' '.join(split_words)   \n",
    "        #new_words = [x if x != 'thier' else 'their' for x in split_words]\n",
    "\n",
    "        # remove nonlinguistic markers\n",
    "        text = remove_markers(text, ['<>', '{}'])\n",
    "\n",
    "        return text\n",
    "\n",
    "    baseline_voc['apostrophe_clean_content'] = baseline_voc.apply(lambda x: apostrophe_clean_within_voc(x['content']), axis=1)\n",
    "    \n",
    "    return baseline_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for number parsing and dollars with punctuation and original uppercasing\n",
    "p = inflect.engine()\n",
    "t2d = text2digits.Text2Digits()\n",
    "\n",
    "def apostrophe_fix_numbers(text):\n",
    "    split_words_num = text.split()\n",
    "    new_list = []\n",
    "    for i in range(len(split_words_num)):\n",
    "        x = split_words_num[i]\n",
    "        \n",
    "        # deal with years\n",
    "        if x.isdigit():\n",
    "            if (1100 <= int(x) < 2000) or (2010 <= int(x) < 2100) or (int(x) == 5050):\n",
    "                # deal with years as colloquially spoken\n",
    "                new_word = p.number_to_words(x[:2]) + \" \" + p.number_to_words(x[2:])\n",
    "            elif \"and\" in p.number_to_words(x):\n",
    "                # remove 'and' from e.g. 'four hundred and ninety five'\n",
    "                output = p.number_to_words(x)\n",
    "                resultwords  = [word for word in output.split() if word not in ['and']]    \n",
    "                new_word = ' '.join(resultwords)\n",
    "            else:\n",
    "                new_word = p.number_to_words(x)\n",
    "            \n",
    "        # deal with cases like 1st, 2nd, etc.\n",
    "        elif re.match(r\"(\\d+)(\\w+)\", x, re.I):\n",
    "            single_digits = ['1st', '2nd', '3rd', '5th', '8th', '9th']\n",
    "            double_digits = ['12th']\n",
    "            single_num = ['1', '2', '3', '5', '8', '9']\n",
    "            double_num = ['12']\n",
    "            single_digit_labels = ['first', 'second', 'third', 'fifth', 'eighth', 'ninth']\n",
    "            double_digit_labels = ['twelfth']\n",
    "            all_digits = single_digits + double_digits\n",
    "            all_labels = single_digit_labels + double_digit_labels\n",
    "            if x in all_digits:\n",
    "                new_word = all_labels[all_digits.index(x)]\n",
    "            else:\n",
    "                items = re.match(r\"(\\d+)(\\w+)\", x, re.I).groups()\n",
    "                if (items[1] not in ['s', 'th', 'st', 'nd', 'rd']):\n",
    "                    new_word = fix_numbers(items[0]) + \" \" + items[1]\n",
    "                elif (items[0][-2:] in double_num):\n",
    "                    new_word = fix_numbers(str(100*int(items[0][:-2]))) + \" \" + fix_numbers(items[0][-2:]+items[1])\n",
    "                elif ((items[0][-1:] in single_num) and items[0][-2:-1] != '1'):\n",
    "                    try:\n",
    "                        new_word = fix_numbers(str(10*int(items[0][:-1]))) + \" \" + fix_numbers(items[0][-1:]+items[1])\n",
    "                    except:\n",
    "                        new_word = fix_numbers(items[0]) + items[1]\n",
    "                # deal with case e.g. 80s\n",
    "                elif (items[1] in ['s', 'th']) and (p.number_to_words(items[0])[-1] == 'y'):\n",
    "                    new_word = fix_numbers(items[0])[:-1] + \"ie\" + items[1]\n",
    "                else:\n",
    "                    new_word = fix_numbers(items[0]) + items[1]\n",
    "                    \n",
    "        # deal with dollars\n",
    "        elif re.match(r\"\\$[^\\]]+\", x, re.I):\n",
    "            # deal with $ to 'dollars'\n",
    "            money = fix_numbers(x[1:])\n",
    "            if x[1:] in [\"1\", \"a\"]:\n",
    "                new_word = money + \" dollar\"\n",
    "            else:\n",
    "                new_word = money + \" dollars\"\n",
    "        else:\n",
    "            new_word = x       \n",
    "        \n",
    "        new_list.append(new_word)\n",
    "        \n",
    "    text = ' '.join(new_list)\n",
    "    text =re.sub(r'[^\\s\\w$\\'\\.\\?\\,\\!]|_', ' ',text)\n",
    "    \n",
    "    # Deal with written out years (two thousand and ten -> twenty ten)\n",
    "    for double_dig in range(10, 100):\n",
    "        double_dig_str = p.number_to_words(double_dig)\n",
    "        text = re.sub('two thousand and ' + double_dig_str, 'twenty ' + double_dig_str, text)\n",
    "        text = re.sub('two thousand ' + double_dig_str, 'twenty ' + double_dig_str, text)\n",
    "        text = re.sub('Two thousand and ' + double_dig_str, 'Twenty ' + double_dig_str, text)\n",
    "        text = re.sub('Two thousand ' + double_dig_str, 'Twenty ' + double_dig_str, text)\n",
    "\n",
    "    # Change e.g. 101 to 'one oh one' -- good for area codes\n",
    "    single_dig_list = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    for j in single_dig_list:\n",
    "        text = re.sub('thousand and ' + j, 'thousand ' + j, text.lower())\n",
    "        for k in single_dig_list:\n",
    "            #print(j + ' hundred ' + k)\n",
    "            text = re.sub(j + ' hundred ' + k + ' ', j + ' oh ' + k + ' ', text.lower())\n",
    "            text = re.sub(j + ' hundred ' + k + '$', j + ' oh ' + k, text.lower())\n",
    "    \n",
    "    text = re.sub(\"\\s+\",\" \",''.join(text)) # standardize whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for state abbreviations with punctuation and original uppercasing\n",
    "\n",
    "def apostrophe_fix_state_abbrevs(text):\n",
    "    ix = 0\n",
    "    state_result = []\n",
    "    wordlist = text.split()\n",
    "    while ix < len(wordlist):\n",
    "        orig_word = wordlist[ix]\n",
    "        word = wordlist[ix].lower().capitalize()\n",
    "        if word in states.keys():\n",
    "            new_word = states[word]\n",
    "        elif (ix < len(wordlist)-1) and ((word + ' ' + wordlist[ix+1].lower().capitalize()) in states.keys()):\n",
    "            new_word = states[(word + ' ' + wordlist[ix+1].lower().capitalize())]\n",
    "            ix += 1\n",
    "        else:\n",
    "            new_word = orig_word\n",
    "        state_result.append(new_word)\n",
    "        ix += 1\n",
    "    text = ' '.join(state_result)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all transcripts\n",
    "\n",
    "def apostrophe_clean_all_transcripts(baseline_snippets):\n",
    "\n",
    "    new_baseline = baseline_snippets.copy()\n",
    "    \n",
    "    swear_words = ['nigga', 'niggas', 'shit', 'bitch', 'damn', 'fuck', 'fuckin', 'fucking', 'motherfuckin', 'motherfucking']\n",
    "    filler_words = ['um', 'uh', 'mm', 'hm', 'ooh', 'woo', 'mhm', 'huh', 'ha']\n",
    "    \n",
    "    pre_cardinal = ['N', 'E', 'S', 'W', 'NE', 'NW', 'SE', 'SW']\n",
    "    post_cardinal = ['North', 'East', 'South', 'West', 'Northeast', 'Northwest', 'Southeast', 'Southwest']\n",
    "    \n",
    "    pre_list = ['cuz', 'ok', 'o', 'till', 'yup', 'imma', 'mister', 'doctor',\n",
    "                'gonna', 'tryna',\n",
    "               'carryout', 'sawmill', 'highschool', 'worldclass',\n",
    "               'theatre', 'neighbour', 'neighbours', 'neighbourhood', 'programme']\n",
    "    post_list = ['cause', 'okay', 'oh', 'til', 'yep', 'ima', 'mr', 'dr',\n",
    "                 'going to', 'trying to',\n",
    "                'carry out', 'saw mill', 'high school', 'world class',\n",
    "                 'theater', 'neighbor', 'neighbors', 'neighborhood', 'program']\n",
    "\n",
    "    def apostrophe_clean_within_all(text):\n",
    "        \n",
    "        # remove hesitation from IBM transcript\n",
    "        text = re.sub('%HESITATION',' ',''.join(text))\n",
    "        \n",
    "        # fix spacing in certain spellings\n",
    "        text = re.sub('T V','TV',''.join(text))\n",
    "        text = re.sub('D C','DC',''.join(text))\n",
    "        \n",
    "        # remove remaining floating non-linguistic words\n",
    "        single_paren = ['<','>', '(',')', '{','}','[',']']\n",
    "        for paren in single_paren:\n",
    "            linguistic_words  = [word for word in text.split() if paren not in word]    \n",
    "            text = ' '.join(linguistic_words)\n",
    "              \n",
    "        # general string cleaning\n",
    "        text = re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", text , 0, re.IGNORECASE) # replace inter-word hyphen with space\n",
    "        #text = re.sub(\"'\",'',''.join(text)) # remove apostrophe\n",
    "        text =re.sub(r'[^\\s\\w$\\'\\.\\?\\,\\!]|_', ' ',text) # replace special characters with space, except $ and apostrophe\n",
    "        text = re.sub(\"\\s+\",\" \",''.join(text)) # standardize whitespace\n",
    "        \n",
    "        # update numeric numbers to strings and remove $\n",
    "        text = re.sub(\"ft ²\", \"square feet\", ''.join(text))\n",
    "        #text = apostrophe_fix_numbers(text)\n",
    "        text = re.sub(\"\\$\",'dollars',''.join(text))\n",
    "        \n",
    "        # standardize spellings\n",
    "        split_words = text.split()\n",
    "        for i in range(len(pre_list)):\n",
    "            split_words = [x if re.sub('\\,','',x.lower()) != pre_list[i] else post_list[i] for x in split_words]\n",
    "        text = ' '.join(split_words)        \n",
    "        \n",
    "        # deal with cardinal directions\n",
    "        split_words_dir = text.split()\n",
    "        for i in range(len(pre_cardinal)):\n",
    "            split_words_dir = [x if re.sub('\\,','',x) != pre_cardinal[i] else post_cardinal[i] for x in split_words_dir]\n",
    "        text = ' '.join(split_words_dir)\n",
    "        \n",
    "        # deal with state abbreviations\n",
    "        text = apostrophe_fix_state_abbrevs(text)\n",
    "        #text = text.lower()\n",
    "   \n",
    "        # update spacing in certain spellings\n",
    "        spacing_list_pre = ['north east', 'north west', 'south east', 'south west', 'all right']\n",
    "        spacing_list_post = ['northeast', 'northwest', 'southeast', 'southwest', 'alright']\n",
    "        for i in range(len(spacing_list_pre)):\n",
    "            text = re.sub(spacing_list_pre[i], spacing_list_post[i],''.join(text))\n",
    "\n",
    "        # remove filler words and swear words\n",
    "        remove_words = swear_words + filler_words\n",
    "        resultwords  = [word for word in text.split() if re.sub('\\,','',word.lower()) not in remove_words]\n",
    "        #resultwords  = [word for word in text.split() if word.lower()[:-1] not in remove_words]\n",
    "        result = ' '.join(resultwords)\n",
    "        \n",
    "        # capitalize first word, remove extra space before comma\n",
    "        result = re.sub(\"\\s+\\,\",\",\",''.join(result))\n",
    "        result = result[0].capitalize() + result[1:]\n",
    "        if result[-1] not in ['.','?','!',',']:\n",
    "            result = result + '.'\n",
    "        if result[-1] == ',':\n",
    "            result = result[:-1] + '.'\n",
    "        \n",
    "        result = re.sub(\"\\s+\\.\",\".\",''.join(result))\n",
    "        result = re.sub(\"\\s+\\!\",\"!\",''.join(result))\n",
    "        result = re.sub(\"\\s+\\?\",\"?\",''.join(result))\n",
    "        \n",
    "        result = re.sub(\" Cause\",\" cause\",''.join(result))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    new_baseline['apostrophe_clean_content'] = new_baseline.apply(lambda x: apostrophe_clean_within_all(x['apostrophe_clean_content']), axis=1)\n",
    "\n",
    "    return new_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply apostrophe-ignoring cleaning rules (for human transcriptions) to the same subset of snippets as determined above\n",
    "\n",
    "apostrophe_coraal = apostrophe_clean_coraal(clean_black_stack)\n",
    "apostrophe_voc = apostrophe_clean_voc(clean_white_stack)\n",
    "apostrophe_usable_input = pd.concat([apostrophe_coraal, apostrophe_voc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply apostrophe-ignoring cleaning rules as done previously, on human transcriptions again\n",
    "\n",
    "apostrophe_clean_usable_snippets = apostrophe_clean_all_transcripts(apostrophe_usable_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export WER to csv for LM perplexity calculation\n",
    "\n",
    "apostrophe_clean_usable_snippets.to_csv(base_folder + 'output/transcribed_wer_usable_punctuation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average WER calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check only\n",
    "# Get average WER for each ASR by race for overall (unmatched) set of samples\n",
    "\n",
    "def calc_avg_wer(df, num, asr_list):\n",
    "    black_mean_list = []\n",
    "    white_mean_list = []\n",
    "    for asr in asr_list:\n",
    "        wer_col = asr+\"_wer\"\n",
    "        black_df = df[df['race_ethnicity']=='Black']\n",
    "        white_df = df[df['race_ethnicity']=='White']\n",
    "        \n",
    "        black_mean = black_df[wer_col].mean()\n",
    "        white_mean = white_df[wer_col].mean()\n",
    "        \n",
    "        black_mean_list.append(black_mean)\n",
    "        white_mean_list.append(white_mean)\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df['ASR'] = asr_list\n",
    "    out_df['Avg_Black_WER'+num] = black_mean_list\n",
    "    out_df['Avg_White_WER'+num] = white_mean_list\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "# Compare WER across different ASR services\n",
    "clean_usable_avg_wer = calc_avg_wer(clean_transcripts_wer, '', clean_asr_trans_list)\n",
    "clean_usable_avg_wer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {
    "8749f5dc7e9345ca820020277385c2a9": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
