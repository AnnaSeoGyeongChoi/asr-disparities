{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain closest matching ASR phrases for n-grams\n",
    "## Determine WER difference between matched samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from nltk import ngrams\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "base_folder = os.getcwd()+'/' #'~/fair-speech/release/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import previously generated transcript WER values\n",
    "\n",
    "clean_transcripts_wer = pd.read_csv(base_folder + 'output/transcribed_wer_usable.csv')\n",
    "clean_transcripts_wer = clean_transcripts_wer.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_transcripts_wer.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into AAVE and White samples\n",
    "\n",
    "def split_white_black(all_trans):\n",
    "    black_stack = all_trans[all_trans['race_ethnicity']=='Black']\n",
    "    white_stack = all_trans[all_trans['race_ethnicity']=='White']\n",
    "    #all_trans['gender'] = np.where(all_trans['female_flag']==1, 'Female', 'Male')\n",
    "    #all_trans['race_ethnicity'] = np.where(all_trans['black_flag']==1, 'Black', 'White')\n",
    "    #black_stack = all_trans[all_trans['black_flag']==1]\n",
    "    #white_stack = all_trans[all_trans['black_flag']==0]\n",
    "    return black_stack, white_stack\n",
    "\n",
    "clean_black_stack, clean_white_stack = split_white_black(clean_transcripts_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams of phrases from given transcriptions\n",
    "\n",
    "def create_ngrams(df, n):\n",
    "    all_grams = []\n",
    "    for row in df.to_records():\n",
    "        grams = ngrams(row.clean_content.split(), n)\n",
    "        for gram in grams:\n",
    "            all_grams.append({\n",
    "                'segment_filename': row.segment_filename,\n",
    "                'basefile': row.basefile,\n",
    "                'race_ethnicity': row.race_ethnicity,\n",
    "                'age': row.age,\n",
    "                'gender': row.gender,\n",
    "                'n': n,\n",
    "                'ngram': ' '.join(gram)\n",
    "            })\n",
    "    return pd.DataFrame(all_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates n-grams of size n until no more common n-grams are found\n",
    "\n",
    "all_grams = pd.DataFrame()\n",
    "make_grams = True\n",
    "n = 5\n",
    "while make_grams:\n",
    "    white_grams = create_ngrams(clean_white_stack, n)\n",
    "    black_grams = create_ngrams(clean_black_stack, n)\n",
    "    commons = set(white_grams.ngram) & set(black_grams.ngram)\n",
    "    white_grams = white_grams[white_grams.ngram.isin(commons)]\n",
    "    black_grams = black_grams[black_grams.ngram.isin(commons)]\n",
    "    all_grams = pd.concat([all_grams, white_grams, black_grams])\n",
    "    \n",
    "    make_grams = len(commons) > 0\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes n-grams that are substrings of larger n-grams\n",
    "\n",
    "is_substring = {}\n",
    "for n in sorted(all_grams.n.unique()):\n",
    "    longer_grams = all_grams[all_grams.n > n]\n",
    "    for ngram, segment_filename in all_grams[all_grams.n == n][['ngram', 'segment_filename']].values:\n",
    "        superstrings = longer_grams[(longer_grams.ngram.str.contains(ngram))\\\n",
    "                                    & (longer_grams.segment_filename == segment_filename)]\n",
    "        is_substring[(segment_filename, ngram)] = len(superstrings) > 0\n",
    "        \n",
    "not_substring = [not is_substring[(sf, g)] for g, sf in all_grams[['ngram', 'segment_filename']].values]\n",
    "all_grams = all_grams[not_substring]\n",
    "all_grams.gender = all_grams.gender.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches ngrams by gender and age, then samples 100 of each race\n",
    "\n",
    "matched_grams = []\n",
    "black_grams = all_grams[all_grams.race_ethnicity == 'Black']\n",
    "white_grams = all_grams[all_grams.race_ethnicity == 'White']\n",
    "age_diff = 5\n",
    "sample_size = 100\n",
    "\n",
    "for row in black_grams.to_records():\n",
    "    match = white_grams[(white_grams.ngram == row.ngram)\\\n",
    "                        & (white_grams.gender == row.gender)\\\n",
    "                        & ((white_grams.age - row.age).abs() <= age_diff)]\n",
    "    if len(match) > 0:\n",
    "        matched_grams.append(row)\n",
    "        matched_grams += list(match.to_records())\n",
    "matched_grams = pd.DataFrame(np.array(matched_grams))\n",
    "matched_grams = matched_grams.drop('index', axis=1).drop_duplicates(subset=['basefile', 'ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matched_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_grams = matched_grams.drop_duplicates(subset=['race_ethnicity', 'ngram'])\n",
    "len(matched_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the phrase in the hypothesis string that best matches the ground truth.\n",
    "# In the case of ties, returns the shortest match\n",
    "\n",
    "def best_phrase(ground_truth, hyp):\n",
    "    ground_truth_len = len(ground_truth.split())\n",
    "    try:\n",
    "        hyp_tokens = hyp.split()\n",
    "    except:\n",
    "        print(hyp)\n",
    "        raise\n",
    "    \n",
    "    opt_phrase = \"\"\n",
    "    opt_phrase_len = math.inf\n",
    "    opt_wer = math.inf\n",
    "    \n",
    "    for start_pos in range(len(hyp_tokens)):\n",
    "        for end_pos in range(start_pos, start_pos+ground_truth_len+1):    \n",
    "            this_phrase = \" \".join(hyp_tokens[start_pos:end_pos])\n",
    "            this_phrase_len = len(this_phrase.split())\n",
    "            this_wer = wer(ground_truth, this_phrase)\n",
    "                    \n",
    "            if (this_wer < opt_wer) or (this_wer == opt_wer and this_phrase_len < opt_phrase_len):\n",
    "                opt_phrase = this_phrase\n",
    "                opt_phrase_len = this_phrase_len\n",
    "                opt_wer = this_wer\n",
    "        \n",
    "    return(opt_phrase, opt_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ngrams to full WER dataset\n",
    "\n",
    "ngrams_with_asr = clean_transcripts_wer.merge(matched_grams,\n",
    "                                              on = ['segment_filename',\n",
    "                                                    'basefile',\n",
    "                                                    'age', \n",
    "                                                    'race_ethnicity'], \n",
    "                                                     how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best phrase for each ASR\n",
    "\n",
    "clean_asr_trans_list = ['clean_google',\n",
    "                        'clean_ibm',\n",
    "                        'clean_amazon',\n",
    "                        'clean_msft',\n",
    "                        'clean_apple']\n",
    "\n",
    "for clean_asr in clean_asr_trans_list:\n",
    "    temp_ngram = ngrams_with_asr.apply(lambda x: best_phrase(x['ngram'], x[clean_asr]), axis=1)\n",
    "    ngrams_with_asr.loc[:, clean_asr+'_phrase'] = temp_ngram.map(lambda x: x[0])\n",
    "    ngrams_with_asr.loc[:, clean_asr+'_ngram_wer'] = temp_ngram.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export n-gram WERs\n",
    "\n",
    "ngrams_with_asr.to_csv(base_folder + 'output/ngrams_with_asr_wer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average ASRs by race\n",
    "\n",
    "ngram_race_breakdown = pd.DataFrame()\n",
    "ngrams_without_inf = ngrams_with_asr.replace([np.inf], 1.0)\n",
    "for clean_asr in clean_asr_trans_list:\n",
    "    x = ngrams_without_inf.groupby(['race_ethnicity'])[clean_asr+'_ngram_wer'].mean().to_frame().transpose()\n",
    "    ngram_race_breakdown = pd.concat([ngram_race_breakdown, x])\n",
    "    \n",
    "ngram_race_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard errors\n",
    "ngram_race_se = pd.DataFrame()\n",
    "for clean_asr in clean_asr_trans_list:\n",
    "    x = ngrams_without_inf.groupby(['race_ethnicity'])[clean_asr+'_ngram_wer'].sem().to_frame().transpose()\n",
    "    ngram_race_se = pd.concat([ngram_race_se, x])\n",
    "    \n",
    "ngram_race_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cleaned table of average n-gram WERs by race\n",
    "\n",
    "ngram_race_breakdown['ASR'] = ['Google', 'IBM', 'Amazon', 'Microsoft', 'Apple']\n",
    "ngram_race_breakdown = ngram_race_breakdown.reset_index(level=0, drop=True)\n",
    "ngram_race_breakdown = ngram_race_breakdown.reindex([4,1,0,2,3])\n",
    "ngram_race_breakdown['Average AAVE WER'] = round(ngram_race_breakdown['Black'],2)\n",
    "ngram_race_breakdown['Average White WER'] = round(ngram_race_breakdown['White'],2)\n",
    "\n",
    "ngram_race_breakdown = ngram_race_breakdown.drop(['Black', 'White'], axis=1)\n",
    "ngram_race_breakdown = ngram_race_breakdown.reset_index(level=0, drop=True)\n",
    "\n",
    "ngram_race_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table of average n-gram WERs as Table 2\n",
    "\n",
    "print(ngram_race_breakdown.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {
    "8749f5dc7e9345ca820020277385c2a9": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
